{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import copy\n","import logging\n","import math\n","import os\n","import random\n","import time\n","from dataclasses import dataclass, field\n","from datetime import datetime\n","from os.path import join as pjoin\n","from typing import Any, Tuple\n","\n","import cv2\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","import pytz\n","import seaborn as sns\n","import torch\n","import torch.nn as nn\n","from PIL import Image\n","from scipy import ndimage\n","from torch import nn\n","from torch.nn import Conv2d, CrossEntropyLoss, Dropout, LayerNorm, Linear, Softmax\n","from torch.nn.modules.utils import _pair\n","from torch.optim.lr_scheduler import LambdaLR\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from torchvision import models, transforms\n","from torchvision.datasets import ImageFolder\n","from tqdm import tqdm"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[],"source":["indonesia_timezone = pytz.timezone(\"Asia/Jakarta\")\n","now = datetime.now(indonesia_timezone)\n","dt_string = now.strftime(\"%d_%m_%Y_%H_%M_%S\")\n","logger = logging.getLogger(__name__)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class Base(object):\n","    def __post_init__(self):\n","        pass\n","\n","\n","@dataclass\n","class DataloaderBaseConfig(Base):\n","    seed: int = 42\n","    batch_size: int = 16\n","    num_workers: int = 4\n","    pin_memory: bool = True\n","    data_dir: str = \"/home/hensel/data\"\n","    weights_dir: str = \"/home/hensel/weights\"\n","    results_dir: str = \"/home/hensel/v2/results\"\n","    models_dir: str = field(init=False)\n","    train_dir: str = field(init=False)\n","    test_dir: str = field(init=False)\n","    cm_csv_dir: str = field(init=False)\n","    cm_acc_csv_dir: str = field(init=False)\n","    attentions_plots_dir: str = field(init=False)\n","    cm_plots_dir: str = field(init=False)\n","    cm_acc_plots_dir: str = field(init=False)\n","    accuracy_plots_dir: str = field(init=False)\n","    loss_plots_dir: str = field(init=False)\n","    train = False\n","    test = True\n","    eval = False\n","    transforms: Any = transforms.ToTensor()\n","\n","    def __post_init__(self):\n","        super().__post_init__()\n","        self.models_dir: str = \"/home/hensel/results/models\"\n","        self.train_dir: str = pjoin(self.results_dir, \"csv/train\")\n","        self.test_dir: str = pjoin(self.results_dir, \"csv/test\")\n","        self.cm_csv_dir: str = pjoin(self.results_dir, \"csv/confusion_matrix\")\n","        self.cm_acc_csv_dir: str = pjoin(self.results_dir, \"csv/confusion_matrix_acc\")\n","        self.attentions_plots_dir: str = pjoin(self.results_dir, \"plots/attentions\")\n","        self.cm_plots_dir: str = pjoin(self.results_dir, \"plots/confusion_matrix\")\n","        self.cm_acc_plots_dir: str = pjoin(\n","            self.results_dir, \"plots/confusion_matrix_acc\"\n","        )\n","        self.accuracy_plots_dir: str = pjoin(self.results_dir, \"plots/training\")\n","        self.loss_plots_dir: str = pjoin(self.results_dir, \"plots/loss\")\n","        os.makedirs(self.models_dir, exist_ok=True)\n","        os.makedirs(self.train_dir, exist_ok=True)\n","        os.makedirs(self.test_dir, exist_ok=True)\n","        os.makedirs(self.cm_csv_dir, exist_ok=True)\n","        os.makedirs(self.cm_acc_csv_dir, exist_ok=True)\n","        os.makedirs(self.attentions_plots_dir, exist_ok=True)\n","        os.makedirs(self.cm_plots_dir, exist_ok=True)\n","        os.makedirs(self.cm_acc_plots_dir, exist_ok=True)\n","        os.makedirs(self.accuracy_plots_dir, exist_ok=True)\n","        os.makedirs(self.loss_plots_dir, exist_ok=True)\n","\n","\n","@dataclass\n","class DataloaderAug(DataloaderBaseConfig):\n","    train_dir: str = field(init=False)\n","    val_dir: str = field(init=False)\n","    test_dir: str = field(init=False)\n","    transforms: Any = transforms.Compose(\n","        [\n","            transforms.Resize((224, 224)),\n","            transforms.RandAugment(),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","        ]\n","    )\n","\n","    def __post_init__(self):\n","        super().__post_init__()\n","        self.train_dir = pjoin(self.data_dir, \"mfn_224_augment_split_mini/train\")\n","        self.val_dir = pjoin(self.data_dir, \"mfn_224_augment_split_mini/val\")\n","        self.test_dir = pjoin(self.data_dir, \"mfn_224_augment_split_mini/test\")\n","\n","\n","@dataclass\n","class DataloaderNonAug(DataloaderBaseConfig):\n","    train_dir: str = field(init=False)\n","    val_dir: str = field(init=False)\n","    test_dir: str = field(init=False)\n","    transforms: Any = transforms.Compose(\n","        [\n","            transforms.Resize((224, 224)),\n","            transforms.ToTensor(),\n","            transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n","        ]\n","    )\n","\n","    def __post_init__(self):\n","        super().__post_init__()\n","        self.train_dir = pjoin(self.data_dir, \"mfn_224_split_mini/train\")\n","        self.val_dir = pjoin(self.data_dir, \"mfn_224_split_mini/val\")\n","        self.test_dir = pjoin(self.data_dir, \"mfn_224_split_mini/test\")\n","\n","\n","@dataclass\n","class VitBaseConfig(Base):\n","    attention_dropout_rate: float = 0.0\n","    dropout_rate: float = 0.1\n","    activation: Any = torch.nn.functional.gelu\n","    img_size: int = 224\n","    in_channels: int = 3\n","    num_classes: int = 4\n","    learning_rate: float = 3e-2\n","    weight_decay: int = 0\n","    momentum: float = 0.9\n","    num_steps: int = 500\n","    warmup_steps: int = 100\n","    max_grad_norm: float = 1.0\n","    seed: int = 42\n","    gradient_accumulation_steps: int = 1\n","    num_epochs: int = 20\n","    early_stop_threshold: int = 0.001\n","    early_stop_patience: int = 3\n","\n","    def __post_init__(self):\n","        super().__post_init__()\n","\n","\n","@dataclass\n","class VitBase(VitBaseConfig):\n","    pretrained_dir: str = field(init=False)\n","    patches: int = (16, 16)\n","    layers: int = 12\n","    hidden_size: int = 768\n","    mlp_size: int = 3072\n","    heads: int = 12\n","\n","    def __post_init__(self):\n","        super().__post_init__()\n","        self.pretrained_dir = pjoin(self.weights_dir, \"ViT-B_16.npz\")\n","\n","\n","@dataclass\n","class VitLarge(VitBaseConfig):\n","    pretrained_dir: str = field(init=False)\n","    patches: int = (16, 16)\n","    layers: int = 24\n","    hidden_size: int = 1024\n","    mlp_size: int = 4096\n","    heads: int = 16\n","\n","    def __post_init__(self):\n","        super().__post_init__()\n","        self.pretrained_dir = pjoin(self.weights_dir, \"ViT-L_16.npz\")\n","\n","\n","@dataclass\n","class VitHuge(VitBaseConfig):\n","    pretrained_dir: str = field(init=False)\n","    patches = (14, 14)\n","    layers = 32\n","    hidden_size = 1280\n","    mlp_size = 5120\n","    heads = 16\n","\n","    def __post_init__(self):\n","        super().__post_init__()\n","        self.pretrained_dir = pjoin(self.weights_dir, \"ViT-H_14.npz\")\n","\n","\n","\"\"\" ViT Base\"\"\"\n","\n","\n","@dataclass\n","class VitBaseAugPretrained(VitBase, DataloaderAug):\n","    name: str = \"vit_base_16_augment_pretrained\"\n","    pretrained: bool = True\n","\n","\n","@dataclass\n","class VitBasePretrained(VitBase, DataloaderNonAug):\n","    name: str = \"vit_base_16_pretrained\"\n","    pretrained: bool = True\n","\n","\n","@dataclass\n","class VitBaseAug(VitBase, DataloaderAug):\n","    name: str = \"vit_base_16_augment\"\n","    pretrained: bool = False\n","\n","\n","@dataclass\n","class VitBase(VitBase, DataloaderNonAug):\n","    name: str = \"vit_base_16\"\n","    pretrained: bool = False\n","\n","\n","\"\"\" ViT Large\"\"\"\n","\n","\n","@dataclass\n","class VitLargeAugPretrained(VitLarge, DataloaderAug):\n","    name: str = \"vit_large_16_augment_pretrained\"\n","    pretrained: bool = True\n","\n","\n","@dataclass\n","class VitLargePretrained(VitLarge, DataloaderNonAug):\n","    name: str = \"vit_large_16_pretrained\"\n","    pretrained: bool = True\n","\n","\n","@dataclass\n","class VitLargeAug(VitLarge, DataloaderAug):\n","    name: str = \"vit_large_16_augment\"\n","    pretrained: bool = False\n","\n","\n","@dataclass\n","class VitLarge(VitLarge, DataloaderNonAug):\n","    name: str = \"vit_large_16\"\n","    pretrained: bool = False\n","\n","\n","\"\"\" ViT Huge\"\"\"\n","\n","\n","@dataclass\n","class VitHugeAugPretrained(VitHuge, DataloaderAug):\n","    name: str = \"vit_huge_14_augment_pretrained\"\n","    pretrained: bool = True\n","\n","\n","@dataclass\n","class VitHugePretrained(VitHuge, DataloaderNonAug):\n","    name: str = \"vit_huge_14_pretrained\"\n","    pretrained: bool = True\n","\n","\n","@dataclass\n","class VitHugeAug(VitHuge, DataloaderAug):\n","    name: str = \"vit_huge_14_augment\"\n","    pretrained: bool = False\n","\n","\n","@dataclass\n","class VitHuge(VitHuge, DataloaderNonAug):\n","    name: str = \"vit_huge_14\"\n","    pretrained: bool = False"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# # Resnet\n","\n","\n","@dataclass\n","class ResnetBaseConfig:\n","    img_size: int = 224\n","    num_classes: int = 4\n","    learning_rate: float = 3e-2\n","    weight_decay: int = 0\n","    momentum: float = 0.9\n","    seed: int = 42\n","    num_epochs: int = 20\n","    early_stop_threshold: int = 0.001\n","    early_stop_patience: int = 3\n","    num_steps: int = 500\n","    warmup_steps: int = 100\n","    max_grad_norm: float = 1.0\n","\n","\n","@dataclass\n","class Resnet152(ResnetBaseConfig, DataloaderNonAug):\n","    name = \"resnet152\"\n","    pretrained = False\n","\n","\n","@dataclass\n","class Resnet152Aug(ResnetBaseConfig, DataloaderAug):\n","    name = \"resnet152_aug\"\n","    pretrained = False\n","\n","\n","@dataclass\n","class Resnet152AugPretrained(ResnetBaseConfig, DataloaderAug):\n","    name = \"resnet152_aug_pretrained\"\n","    pretrained = True\n","\n","\n","@dataclass\n","class Resnet152Pretrained(ResnetBaseConfig, DataloaderNonAug):\n","    name = \"resnet152_pretrained\"\n","    pretrained = True\n","\n","\n","@dataclass\n","class Resnet50(ResnetBaseConfig, DataloaderNonAug):\n","    name = \"resnet50\"\n","    pretrained = False\n","\n","\n","@dataclass\n","class Resnet50Aug(ResnetBaseConfig, DataloaderAug):\n","    name = \"resnet50_aug\"\n","    pretrained = False\n","\n","\n","@dataclass\n","class Resnet50AugPretrained(ResnetBaseConfig, DataloaderAug):\n","    name = \"resnet50_aug_pretrained\"\n","    pretrained = True\n","\n","\n","@dataclass\n","class Resnet50Pretrained(ResnetBaseConfig, DataloaderNonAug):\n","    name = \"resnet50_pretrained\"\n","    pretrained = True\n","\n","\n","def np2th(weights):\n","    return torch.from_numpy(weights)\n","\n","\n","class Attention(nn.Module):\n","    def __init__(self, config):\n","        super(Attention, self).__init__()\n","        self.num_attention_heads = config.heads\n","        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n","        self.all_head_size = self.num_attention_heads * self.attention_head_size\n","\n","        self.query = Linear(config.hidden_size, self.all_head_size)\n","        self.key = Linear(config.hidden_size, self.all_head_size)\n","        self.value = Linear(config.hidden_size, self.all_head_size)\n","\n","        self.out = Linear(config.hidden_size, config.hidden_size)\n","        self.attn_dropout = Dropout(config.attention_dropout_rate)\n","        self.proj_dropout = Dropout(config.attention_dropout_rate)\n","\n","        self.softmax = Softmax(dim=-1)\n","\n","    def transpose_for_scores(self, x):\n","        new_x_shape = x.size()[:-1] + (\n","            self.num_attention_heads,\n","            self.attention_head_size,\n","        )\n","        x = x.view(*new_x_shape)\n","        return x.permute(0, 2, 1, 3)\n","\n","    def forward(self, hidden_states):\n","        mixed_query_layer = self.query(hidden_states)\n","        mixed_key_layer = self.key(hidden_states)\n","        mixed_value_layer = self.value(hidden_states)\n","\n","        query_layer = self.transpose_for_scores(mixed_query_layer)\n","        key_layer = self.transpose_for_scores(mixed_key_layer)\n","        value_layer = self.transpose_for_scores(mixed_value_layer)\n","\n","        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n","        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n","        attention_probs = self.softmax(attention_scores)\n","        weights = attention_probs\n","        attention_probs = self.attn_dropout(attention_probs)\n","\n","        context_layer = torch.matmul(attention_probs, value_layer)\n","        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n","        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n","        context_layer = context_layer.view(*new_context_layer_shape)\n","        attention_output = self.out(context_layer)\n","        attention_output = self.proj_dropout(attention_output)\n","        return attention_output, weights\n","\n","\n","class Mlp(nn.Module):\n","    def __init__(self, config):\n","        super(Mlp, self).__init__()\n","        self.fc1 = Linear(config.hidden_size, config.mlp_size)\n","        self.fc2 = Linear(config.mlp_size, config.hidden_size)\n","        self.act_fn = torch.nn.functional.gelu\n","        self.dropout = Dropout(config.attention_dropout_rate)\n","\n","        self._init_weights()\n","\n","    def _init_weights(self):\n","        nn.init.xavier_uniform_(self.fc1.weight)\n","        nn.init.xavier_uniform_(self.fc2.weight)\n","        nn.init.normal_(self.fc1.bias, std=1e-6)\n","        nn.init.normal_(self.fc2.bias, std=1e-6)\n","\n","    def forward(self, x):\n","        x = self.fc1(x)\n","        x = self.act_fn(x)\n","        x = self.dropout(x)\n","        x = self.fc2(x)\n","        x = self.dropout(x)\n","        return x\n","\n","\n","class Embeddings(nn.Module):\n","    def __init__(self, config):\n","        super(Embeddings, self).__init__()\n","        self.hybrid = None\n","        img_size = _pair(config.img_size)\n","\n","        patch_size = _pair(config.patches)\n","        n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n","        self.hybrid = False\n","\n","        self.patch_embeddings = Conv2d(\n","            in_channels=config.in_channels,\n","            out_channels=config.hidden_size,\n","            kernel_size=patch_size,\n","            stride=patch_size,\n","        )\n","        self.position_embeddings = nn.Parameter(\n","            torch.zeros(1, n_patches + 1, config.hidden_size)\n","        )\n","        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n","\n","        self.dropout = Dropout(config.attention_dropout_rate)\n","\n","    def forward(self, x):\n","        B = x.shape[0]\n","        cls_tokens = self.cls_token.expand(B, -1, -1)\n","        x = self.patch_embeddings(x)\n","        x = x.flatten(2)\n","        x = x.transpose(-1, -2)\n","        x = torch.cat((cls_tokens, x), dim=1)\n","\n","        embeddings = x + self.position_embeddings\n","        embeddings = self.dropout(embeddings)\n","        return embeddings\n","\n","\n","class Block(nn.Module):\n","    def __init__(self, config):\n","        super(Block, self).__init__()\n","        self.hidden_size = config.hidden_size\n","        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n","        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n","        self.ffn = Mlp(config)\n","        self.attn = Attention(config)\n","\n","    def forward(self, x):\n","        h = x\n","        x = self.attention_norm(x)\n","        x, weights = self.attn(x)\n","        x = x + h\n","\n","        h = x\n","        x = self.ffn_norm(x)\n","        x = self.ffn(x)\n","        x = x + h\n","        return x, weights\n","\n","    def load_from(self, weights, n_block):\n","        with torch.no_grad():\n","            query_weight = (\n","                np2th(\n","                    weights[\n","                        pjoin(\n","                            \"Transformer/encoderblock_\" + n_block,\n","                            \"MultiHeadDotProductAttention_1/query\",\n","                            \"kernel\",\n","                        )\n","                    ]\n","                )\n","                .view(self.hidden_size, self.hidden_size)\n","                .t()\n","            )\n","            key_weight = (\n","                np2th(\n","                    weights[\n","                        pjoin(\n","                            \"Transformer/encoderblock_\" + n_block,\n","                            \"MultiHeadDotProductAttention_1/key\",\n","                            \"kernel\",\n","                        )\n","                    ]\n","                )\n","                .view(self.hidden_size, self.hidden_size)\n","                .t()\n","            )\n","            value_weight = (\n","                np2th(\n","                    weights[\n","                        pjoin(\n","                            \"Transformer/encoderblock_\" + n_block,\n","                            \"MultiHeadDotProductAttention_1/value\",\n","                            \"kernel\",\n","                        )\n","                    ]\n","                )\n","                .view(self.hidden_size, self.hidden_size)\n","                .t()\n","            )\n","            out_weight = (\n","                np2th(\n","                    weights[\n","                        pjoin(\n","                            \"Transformer/encoderblock_\" + n_block,\n","                            \"MultiHeadDotProductAttention_1/out\",\n","                            \"kernel\",\n","                        )\n","                    ]\n","                )\n","                .view(self.hidden_size, self.hidden_size)\n","                .t()\n","            )\n","\n","            query_bias = np2th(\n","                weights[\n","                    pjoin(\n","                        \"Transformer/encoderblock_\" + n_block,\n","                        \"MultiHeadDotProductAttention_1/query\",\n","                        \"bias\",\n","                    )\n","                ]\n","            ).view(-1)\n","            key_bias = np2th(\n","                weights[\n","                    pjoin(\n","                        \"Transformer/encoderblock_\" + n_block,\n","                        \"MultiHeadDotProductAttention_1/key\",\n","                        \"bias\",\n","                    )\n","                ]\n","            ).view(-1)\n","            value_bias = np2th(\n","                weights[\n","                    pjoin(\n","                        \"Transformer/encoderblock_\" + n_block,\n","                        \"MultiHeadDotProductAttention_1/value\",\n","                        \"bias\",\n","                    )\n","                ]\n","            ).view(-1)\n","            out_bias = np2th(\n","                weights[\n","                    pjoin(\n","                        \"Transformer/encoderblock_\" + n_block,\n","                        \"MultiHeadDotProductAttention_1/out\",\n","                        \"bias\",\n","                    )\n","                ]\n","            ).view(-1)\n","\n","            self.attn.query.weight.copy_(query_weight)\n","            self.attn.key.weight.copy_(key_weight)\n","            self.attn.value.weight.copy_(value_weight)\n","            self.attn.out.weight.copy_(out_weight)\n","            self.attn.query.bias.copy_(query_bias)\n","            self.attn.key.bias.copy_(key_bias)\n","            self.attn.value.bias.copy_(value_bias)\n","            self.attn.out.bias.copy_(out_bias)\n","\n","            mlp_weight_0 = np2th(\n","                weights[\n","                    pjoin(\n","                        \"Transformer/encoderblock_\" + n_block,\n","                        \"MlpBlock_3/Dense_0\",\n","                        \"kernel\",\n","                    )\n","                ]\n","            ).t()\n","            mlp_weight_1 = np2th(\n","                weights[\n","                    pjoin(\n","                        \"Transformer/encoderblock_\" + n_block,\n","                        \"MlpBlock_3/Dense_1\",\n","                        \"kernel\",\n","                    )\n","                ]\n","            ).t()\n","            mlp_bias_0 = np2th(\n","                weights[\n","                    pjoin(\n","                        \"Transformer/encoderblock_\" + n_block,\n","                        \"MlpBlock_3/Dense_0\",\n","                        \"bias\",\n","                    )\n","                ]\n","            ).t()\n","            mlp_bias_1 = np2th(\n","                weights[\n","                    pjoin(\n","                        \"Transformer/encoderblock_\" + n_block,\n","                        \"MlpBlock_3/Dense_1\",\n","                        \"bias\",\n","                    )\n","                ]\n","            ).t()\n","\n","            self.ffn.fc1.weight.copy_(mlp_weight_0)\n","            self.ffn.fc2.weight.copy_(mlp_weight_1)\n","            self.ffn.fc1.bias.copy_(mlp_bias_0)\n","            self.ffn.fc2.bias.copy_(mlp_bias_1)\n","\n","            self.attention_norm.weight.copy_(\n","                np2th(\n","                    weights[\n","                        pjoin(\n","                            \"Transformer/encoderblock_\" + n_block,\n","                            \"LayerNorm_0\",\n","                            \"scale\",\n","                        )\n","                    ]\n","                )\n","            )\n","            self.attention_norm.bias.copy_(\n","                np2th(\n","                    weights[\n","                        pjoin(\n","                            \"Transformer/encoderblock_\" + n_block, \"LayerNorm_0\", \"bias\"\n","                        )\n","                    ]\n","                )\n","            )\n","            self.ffn_norm.weight.copy_(\n","                np2th(\n","                    weights[\n","                        pjoin(\n","                            \"Transformer/encoderblock_\" + n_block,\n","                            \"LayerNorm_2\",\n","                            \"scale\",\n","                        )\n","                    ]\n","                )\n","            )\n","            self.ffn_norm.bias.copy_(\n","                np2th(\n","                    weights[\n","                        pjoin(\n","                            \"Transformer/encoderblock_\" + n_block, \"LayerNorm_2\", \"bias\"\n","                        )\n","                    ]\n","                )\n","            )\n","\n","\n","class Encoder(nn.Module):\n","    def __init__(self, config):\n","        super(Encoder, self).__init__()\n","        self.layer = nn.ModuleList()\n","        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n","        for _ in range(config.layers):\n","            layer = Block(config)\n","            self.layer.append(copy.deepcopy(layer))\n","\n","    def forward(self, hidden_states):\n","        attn_weights = []\n","        for layer_block in self.layer:\n","            hidden_states, weights = layer_block(hidden_states)\n","            attn_weights.append(weights)\n","        encoded = self.encoder_norm(hidden_states)\n","        return encoded, attn_weights\n","\n","\n","class Transformer(nn.Module):\n","    def __init__(self, config):\n","        super(Transformer, self).__init__()\n","        self.embeddings = Embeddings(config)\n","        self.encoder = Encoder(config)\n","\n","    def forward(self, input_ids):\n","        embedding_output = self.embeddings(input_ids)\n","        encoded, attn_weights = self.encoder(embedding_output)\n","        return encoded, attn_weights\n","\n","\n","class VisionTransformer(nn.Module):\n","    def __init__(self, config):\n","        super(VisionTransformer, self).__init__()\n","        self.num_classes = config.num_classes\n","\n","        self.transformer = Transformer(config)\n","        self.head = Linear(config.hidden_size, config.num_classes)\n","\n","    def forward(self, x):\n","        x, attn_weights = self.transformer(x)\n","        logits = self.head(x[:, 0])\n","        return logits, attn_weights\n","\n","    def load_from(self, weights):\n","        with torch.no_grad():\n","            nn.init.zeros_(self.head.weight)\n","            nn.init.zeros_(self.head.bias)\n","\n","            self.transformer.embeddings.patch_embeddings.weight.copy_(\n","                np2th(weights[\"embedding/kernel\"].transpose([3, 2, 0, 1]))\n","            )\n","            self.transformer.embeddings.patch_embeddings.bias.copy_(\n","                np2th(weights[\"embedding/bias\"])\n","            )\n","            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n","            self.transformer.encoder.encoder_norm.weight.copy_(\n","                np2th(weights[\"Transformer/encoder_norm/scale\"])\n","            )\n","            self.transformer.encoder.encoder_norm.bias.copy_(\n","                np2th(weights[\"Transformer/encoder_norm/bias\"])\n","            )\n","\n","            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n","            posemb_new = self.transformer.embeddings.position_embeddings\n","            if posemb.size() == posemb_new.size():\n","                self.transformer.embeddings.position_embeddings.copy_(posemb)\n","            else:\n","                logger.info(\n","                    \"load_pretrained: resized variant: %s to %s\"\n","                    % (posemb.size(), posemb_new.size())\n","                )\n","                ntok_new = posemb_new.size(1)\n","\n","                posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n","                ntok_new -= 1\n","\n","                gs_old = int(np.sqrt(len(posemb_grid)))\n","                gs_new = int(np.sqrt(ntok_new))\n","                print(\"load_pretrained: grid-size from %s to %s\" % (gs_old, gs_new))\n","                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n","\n","                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n","                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n","                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n","                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n","                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n","\n","            for bname, block in self.transformer.encoder.named_children():\n","                for uname, unit in block.named_children():\n","                    unit.load_from(weights, n_block=uname)"]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["class WarmupCosineSchedule(LambdaLR):\n","    def __init__(self, optimizer, warmup_steps, t_total, cycles=0.5, last_epoch=-1):\n","        self.warmup_steps = warmup_steps\n","        self.t_total = t_total\n","        self.cycles = cycles\n","        super(WarmupCosineSchedule, self).__init__(\n","            optimizer, self.lr_lambda, last_epoch=last_epoch\n","        )\n","\n","    def lr_lambda(self, step):\n","        if step < self.warmup_steps:\n","            return float(step) / float(max(1.0, self.warmup_steps))\n","        progress = float(step - self.warmup_steps) / float(\n","            max(1, self.t_total - self.warmup_steps)\n","        )\n","        return max(\n","            0.0,\n","            0.5 * (1.0 + math.cos(math.pi * float(self.cycles) * 2.0 * progress)),\n","        )"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["def get_loader(\n","    config: dataclass,\n",") -> Tuple[DataLoader, DataLoader, DataLoader]:\n","\n","    trainset = ImageFolder(root=config.train_dir, transform=config.transforms)\n","    valset = ImageFolder(root=config.val_dir, transform=config.transforms)\n","    testset = ImageFolder(root=config.test_dir, transform=config.transforms)\n","\n","    trainset_sampler = RandomSampler(trainset)\n","    valset_sampler = SequentialSampler(valset)\n","    testset_sampler = SequentialSampler(testset)\n","\n","    train_loader = torch.utils.data.DataLoader(\n","        dataset=trainset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        pin_memory=config.pin_memory,\n","        sampler=trainset_sampler,\n","    )\n","    val_loader = torch.utils.data.DataLoader(\n","        dataset=valset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        pin_memory=config.pin_memory,\n","        sampler=valset_sampler,\n","    )\n","    test_loader = torch.utils.data.DataLoader(\n","        dataset=testset,\n","        batch_size=config.batch_size,\n","        num_workers=config.num_workers,\n","        pin_memory=config.pin_memory,\n","        sampler=testset_sampler,\n","    )\n","\n","    return train_loader, val_loader, test_loader"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["class Metrics:\n","    def __init__(self, config):\n","        self.config = config\n","        self.metrics = {\n","            \"epoch\": [],\n","            \"train_time\": [],\n","            \"train_accuracy\": [],\n","            \"train_loss\": [],\n","            \"eval_time\": [],\n","            \"eval_accuracy\": [],\n","            \"eval_loss\": [],\n","        }\n","        self.epoch = 0\n","\n","    def reset(self):\n","        self.train_time = 0\n","        self.train_accuracy = 0\n","        self.train_loss = 0\n","        self.eval_time = 0\n","        self.eval_accuracy = 0\n","        self.eval_loss = 0\n","\n","    def train_update(self, train_time, accuracy, loss):\n","        self.reset()\n","        self.train_time = train_time\n","        self.metrics[\"train_time\"].append(self.train_time)\n","\n","        self.train_accuracy = accuracy\n","        self.train_loss = loss\n","\n","        self.metrics[\"train_accuracy\"].append(accuracy)\n","        self.metrics[\"train_loss\"].append(loss)\n","\n","    def eval_update(self, eval_time, accuracy, loss):\n","        self.reset()\n","\n","        self.epoch += 1\n","        self.metrics[\"epoch\"].append(self.epoch)\n","\n","        self.eval_time = eval_time\n","        self.metrics[\"eval_time\"].append(self.eval_time)\n","\n","        self.eval_accuracy = accuracy\n","        self.eval_loss = loss\n","\n","        self.metrics[\"eval_accuracy\"].append(accuracy)\n","        self.metrics[\"eval_loss\"].append(loss)\n","\n","    def to_pandas(self):\n","        metrics = self.metrics\n","        df = pd.DataFrame(metrics)\n","        df = df.add_prefix(f\"{self.config.name}_\")\n","        return df\n","\n","    def to_csv(self):\n","        df = self.to_pandas()\n","        df.to_csv(f\"{self.config.train_dir}/{self.config.name}.csv\", index=False)\n","\n","    def to_plot(self):\n","        df = self.to_pandas()\n","        title = f\"{self.config.name}\"\n","        title = title.split(\"_\")\n","        title = \" \".join(title).title()\n","\n","        ax1 = df[[x for x in df.columns if \"loss\" not in x and \"time\" not in x]].plot(\n","            x=f\"{self.config.name}_epoch\",\n","            figsize=(15, 10),\n","            linewidth=5,\n","            kind=\"line\",\n","            legend=True,\n","            fontsize=16,\n","        )\n","        ax1.legend(\n","            loc=\"lower right\",\n","            prop={\"size\": 20},\n","        )\n","        ax1.grid(True)\n","        ax1.set_title(title + \" Accuracy\", fontsize=30)\n","        ax1.set_xlabel(\"Epoch\", fontsize=20)\n","        ax1.set_ylabel(\"Accuracy\", fontsize=20)\n","        ax1.set_xticks(np.arange(1, 21))\n","        ax1.set_yticks(np.arange(0, 1.1, 0.1))\n","        ax1.grid(which=\"major\", color=\"#CCCCCC\", linestyle=\"--\")\n","        ax1.grid(which=\"minor\", color=\"#CCCCCC\", linestyle=\":\")\n","        ax1.figure.savefig(\n","            self.config.accuracy_plots_dir + \"/\" + self.config.name + \"_accuracy.png\"\n","        )\n","\n","        ax2 = df[\n","            [x for x in df.columns if \"accuracy\" not in x and \"time\" not in x]\n","        ].plot(\n","            x=f\"{self.config.name}_epoch\",\n","            figsize=(15, 10),\n","            linewidth=5,\n","            kind=\"line\",\n","            legend=True,\n","            fontsize=16,\n","        )\n","        ax2.legend(\n","            loc=\"lower right\",\n","            prop={\"size\": 20},\n","        )\n","        ax2.grid(True)\n","        ax2.set_xlabel(\"Epoch\", fontsize=20)\n","        ax2.set_ylabel(\"Loss\", fontsize=20)\n","        ax2.set_title(title + \" Loss\", fontsize=30)\n","        ax2.set_xticks(np.arange(1, 21))\n","        ax2.grid(which=\"major\", color=\"#CCCCCC\", linestyle=\"--\")\n","        ax2.grid(which=\"minor\", color=\"#CCCCCC\", linestyle=\":\")\n","        ax2.figure.savefig(\n","            self.config.loss_plots_dir + \"/\" + self.config.name + \"_loss.png\"\n","        )\n","        plt.close(\"all\")\n","\n","    def get_metrics(self):\n","        return self.metrics\n","\n","    def early_stop(self, model):\n","        if self.eval_best_loss - self.eval_loss > self.early_stop_threshold:\n","            best_model = copy.deepcopy(model)\n","            model_dir = (\n","                f\"{self.config.models_dir}/{self.config.name}_Early_Stopped_Model.pt\"\n","            )\n","            if os.path.exists(model_dir):\n","                os.remove(model_dir)\n","\n","            torch.save(best_model.state_dict(), model_dir)\n","            self.eval_best_loss = self.eval_loss\n","            self.early_stop_counter = 0\n","\n","        elif self.eval_best_loss - self.eval_loss < self.early_stop_threshold:\n","            self.early_stop_counter += 1\n","            if self.early_stop_counter >= self.early_stop_patience:\n","                return True\n","\n","\n","def run(config, model):\n","    train_loader, eval_loader, test_loader = get_loader(config)\n","\n","    optimizer = torch.optim.SGD(\n","        model.parameters(),\n","        lr=config.learning_rate,\n","        momentum=config.momentum,\n","        weight_decay=config.weight_decay,\n","    )\n","\n","    t_total = config.num_steps\n","\n","    scheduler = WarmupCosineSchedule(\n","        optimizer, warmup_steps=config.warmup_steps, t_total=t_total\n","    )\n","\n","    metrics = Metrics(config)\n","\n","    random.seed(config.seed)\n","    np.random.seed(config.seed)\n","    torch.manual_seed(config.seed)\n","\n","    best_acc = 0.0\n","    if config.train:\n","        for epoch in range(config.num_epochs):\n","            model.train()\n","            train_iterator = tqdm(\n","                train_loader,\n","                desc=config.name\n","                + \" Training Epoch X / X : Batch X / X) (Acc = X, Loss = X)\",\n","                bar_format=\"{l_bar}{r_bar}\",\n","                dynamic_ncols=True,\n","            )\n","            loss_fct = CrossEntropyLoss()\n","\n","            train_running_loss = 0.0\n","            train_running_corrects = 0.0\n","\n","            train_epoch_acc = 0.0\n","            train_epoch_loss = 0.0\n","\n","            eval_running_loss = 0.0\n","            eval_running_corrects = 0.0\n","\n","            eval_epoch_loss = 0.0\n","            eval_epoch_acc = 0.0\n","\n","            train_start_time = time.process_time()\n","\n","            for batch, (inputs, labels) in enumerate(train_iterator):\n","\n","                inputs = inputs.to(config.device)\n","                labels = labels.to(config.device)\n","\n","                optimizer.zero_grad()\n","\n","                if \"vit\" in config.name:\n","                    logits, attn_weights = model(inputs)\n","                else:\n","                    logits = model(inputs)\n","                loss = loss_fct(logits.view(-1, config.num_classes), labels.view(-1))\n","\n","                _, preds = torch.max(logits, 1)\n","                train_running_acc = (preds == labels).sum() / len(labels)\n","\n","                train_running_loss += loss.item() * inputs.size(0)\n","                train_running_corrects += torch.sum(preds == labels.data)\n","\n","                loss.backward()\n","                optimizer.step()\n","                scheduler.step()\n","\n","                torch.nn.utils.clip_grad_norm_(model.parameters(), config.max_grad_norm)\n","\n","                batch_cnt = batch + 1\n","                epoch_cnt = epoch + 1\n","\n","                train_iterator.set_description(\n","                    config.name\n","                    + \" Training Epoch %d / %d : Batch %d / %d) (Acc = %2.5f, Loss = %2.5f)\"\n","                    % (\n","                        epoch_cnt,\n","                        config.num_epochs,\n","                        batch_cnt,\n","                        len(train_loader),\n","                        train_running_acc.item(),\n","                        loss.item(),\n","                    )\n","                )\n","            train_epoch_loss = train_running_loss / len(train_loader.dataset)\n","            train_epoch_acc = (\n","                train_running_corrects / len(train_loader.dataset)\n","            ).item()\n","\n","            train_end_time = time.process_time() - train_start_time\n","            metrics.train_update(train_end_time, train_epoch_acc, train_epoch_loss)\n","        if config.eval:\n","            model.eval()\n","            eval_iterator = tqdm(\n","                eval_loader,\n","                desc=config.name\n","                + \" Eval Epoch X / X : Batch X / X) (Acc = X, Loss = X)\",\n","                bar_format=\"{l_bar}{r_bar}\",\n","                dynamic_ncols=True,\n","            )\n","            eval_start_time = time.process_time()\n","            for batch, (inputs, labels) in enumerate(eval_iterator):\n","                inputs = inputs.to(config.device)\n","                labels = labels.to(config.device)\n","\n","                with torch.no_grad():\n","                    if \"vit\" in config.name:\n","                        logits, atttn_weights = model(inputs)\n","                    else:\n","                        logits = model(inputs)\n","                    loss = loss_fct(logits, labels)\n","                    _, preds = torch.max(logits, 1)\n","\n","                eval_running_acc = ((preds == labels).sum() / len(labels)).item()\n","\n","                eval_running_loss += loss.item() * inputs.size(0)\n","                eval_running_corrects += torch.sum(preds == labels.data)\n","\n","                batch_cnt = batch + 1\n","                epoch_cnt = epoch + 1\n","\n","                eval_iterator.set_description(\n","                    config.name\n","                    + \" Eval Epoch %d / %d : Batch %d / %d) (Acc = %2.5f, Loss = %2.5f)\"\n","                    % (\n","                        epoch_cnt,\n","                        config.num_epochs,\n","                        batch_cnt,\n","                        len(eval_loader),\n","                        eval_running_acc,\n","                        loss.item(),\n","                    )\n","                )\n","\n","            eval_epoch_loss = eval_running_loss / len(eval_loader.dataset)\n","            eval_epoch_acc = (eval_running_corrects / len(eval_loader.dataset)).item()\n","            eval_end_time = time.process_time() - eval_start_time\n","\n","        metrics.eval_update(eval_end_time, eval_epoch_acc, eval_epoch_loss)\n","        metrics.to_csv()\n","        metrics.to_plot()\n","\n","        if eval_epoch_acc > best_acc:\n","\n","            best_acc = eval_epoch_acc\n","            best_model = copy.deepcopy(model)\n","            model_dir = f\"{config.models_dir}/{config.name}_best_model.pth\"\n","            if os.path.exists(model_dir):\n","                os.remove(model_dir)\n","\n","            torch.save(\n","                {\n","                    \"epoch\": epoch,\n","                    \"model_state_dict\": best_model.state_dict(),\n","                    \"optimizer_state_dict\": optimizer.state_dict(),\n","                    \"loss\": eval_epoch_loss,\n","                },\n","                model_dir,\n","            )\n","            logger.info(f\"Best accuracy at {best_acc}\")\n","            logger.info(f\"Saved model to {model_dir}\")\n","\n","    if config.test:\n","        test(config, model, test_loader)\n","\n","\n","def test(config, model, test_loader):\n","\n","    classes = test_loader.dataset.classes\n","\n","    metrics = {\n","        \"test_acc\": [],\n","        \"test_loss\": [],\n","        \"test_time\": [],\n","    }\n","\n","    test_iterator = tqdm(\n","        test_loader,\n","        desc=config.name + \" Test Epoch X / X : Batch X / X) (Acc = X, Loss = X)\",\n","        bar_format=\"{l_bar}{r_bar}\",\n","        dynamic_ncols=True,\n","    )\n","    classes = test_loader.dataset.classes\n","\n","    loss_fct = CrossEntropyLoss()\n","    test_running_loss = 0.0\n","    test_running_corrects = 0.0\n","\n","    test_epoch_loss = 0.0\n","    test_epoch_acc = 0.0\n","\n","    checkpoint = torch.load(config.models_dir + \"/\" + config.name + \"_best_model.pth\")\n","\n","    model.load_state_dict(checkpoint[\"model_state_dict\"])\n","    model.eval()\n","    model.to(config.device)\n","\n","    nb_classes = len(classes)\n","    confusion_matrix = np.zeros((nb_classes, nb_classes))\n","\n","    test_start_time = time.process_time()\n","    for batch, (inputs, labels) in enumerate(test_iterator):\n","        inputs = inputs.to(config.device)\n","        labels = labels.to(config.device)\n","\n","        with torch.no_grad():\n","            if \"vit\" in config.name:\n","                logits, atttn_weights = model(inputs)\n","            else:\n","                logits = model(inputs)\n","            loss = loss_fct(logits, labels)\n","            _, preds = torch.max(logits, 1)\n","            for t, p in zip(labels.view(-1), preds.view(-1)):\n","                confusion_matrix[t.long(), p.long()] += 1\n","\n","        batch_cnt = batch + 1\n","        test_running_acc = ((preds == labels).sum() / len(labels)).item()\n","        test_running_loss += loss.item() * inputs.size(0)\n","        test_running_corrects += torch.sum(preds == labels.data)\n","\n","        test_iterator.set_description(\n","            config.name\n","            + \" Test Batch %d / %d) (Acc = %2.5f, Loss = %2.5f)\"\n","            % (\n","                batch_cnt,\n","                len(test_loader),\n","                test_running_acc,\n","                loss.item(),\n","            )\n","        )\n","    test_epoch_loss = test_running_loss / len(test_loader.dataset)\n","    test_epoch_acc = (test_running_corrects / len(test_loader.dataset)).item()\n","\n","    test_end_time = time.process_time() - test_start_time\n","\n","    metrics[\"test_acc\"].append(test_epoch_acc)\n","    metrics[\"test_loss\"].append(test_epoch_loss)\n","    metrics[\"test_time\"].append(test_end_time)\n","\n","    pd.DataFrame(metrics).to_csv(\n","        f\"{config.test_dir}/{config.name}_test.csv\", index=False\n","    )\n","\n","    plt.figure(figsize=(15, 15))\n","\n","    df_cm = pd.DataFrame(confusion_matrix, index=classes, columns=classes).astype(int)\n","\n","    df_cm_acc = df_cm.copy(deep=True)\n","    for class_name in classes:\n","        df_cm_acc[class_name] = df_cm_acc[class_name] / df_cm_acc[class_name].sum()\n","\n","    df_cm.to_csv(f\"{config.cm_csv_dir}/{config.name}_cm.csv\", index=False)\n","    df_cm_acc.to_csv(f\"{config.cm_acc_csv_dir}/{config.name}_cm_acc.csv\", index=False)\n","\n","    heatmap = sns.heatmap(df_cm, annot=True)\n","    heatmap.yaxis.set_ticklabels(\n","        heatmap.yaxis.get_ticklabels(), rotation=0, ha=\"right\", fontsize=15\n","    )\n","    heatmap.xaxis.set_ticklabels(\n","        heatmap.xaxis.get_ticklabels(), rotation=45, ha=\"right\", fontsize=15\n","    )\n","\n","    plt.ylabel(\"True label\")\n","    plt.xlabel(\"Predicted label\")\n","    plt.savefig(config.cm_plots_dir + \"/\" + config.name + \"_confusion_matrix.png\")\n","    plt.close(\"all\")\n","\n","    heatmap = sns.heatmap(df_cm_acc, annot=True)\n","    heatmap.yaxis.set_ticklabels(\n","        heatmap.yaxis.get_ticklabels(), rotation=0, ha=\"right\", fontsize=15\n","    )\n","    heatmap.xaxis.set_ticklabels(\n","        heatmap.xaxis.get_ticklabels(), rotation=45, ha=\"right\", fontsize=15\n","    )\n","\n","    plt.ylabel(\"True label\")\n","    plt.xlabel(\"Predicted label\")\n","    plt.savefig(\n","        config.cm_acc_plots_dir + \"/\" + config.name + \"_acc_confusion_matrix.png\"\n","    )\n","    plt.close(\"all\")\n","\n","    if \"vit\" in config.name:\n","        transform = transforms.Compose(\n","            [transforms.Resize((224, 224)), transforms.ToTensor()]\n","        )\n","        rand_class = np.random.choice(classes)\n","        list_of_images = os.listdir(pjoin(config.val_dir, rand_class))\n","        rand_image = np.random.choice(list_of_images)\n","\n","        im = Image.open(pjoin(pjoin(config.val_dir, rand_class), rand_image))\n","\n","        x = transform(im)\n","\n","        model.to(torch.device(\"cpu\"))\n","        logits, att_mat = model(x.unsqueeze(0))\n","\n","        att_mat = torch.stack(att_mat).squeeze(1)\n","        att_mat = torch.mean(att_mat, dim=1)\n","\n","        residual_att = torch.eye(att_mat.size(1))\n","\n","        aug_att_mat = att_mat + residual_att\n","        aug_att_mat = aug_att_mat / aug_att_mat.sum(dim=-1).unsqueeze(-1)\n","\n","        joint_attentions = torch.zeros(aug_att_mat.size())\n","        joint_attentions[0] = aug_att_mat[0]\n","\n","        for n in range(1, aug_att_mat.size(0)):\n","            joint_attentions[n] = torch.matmul(aug_att_mat[n], joint_attentions[n - 1])\n","\n","        v = joint_attentions[-1]\n","\n","        grid_size = int(np.sqrt(aug_att_mat.size(-1)))\n","\n","        mask = v[0, 1:].reshape(grid_size, grid_size).detach().numpy()\n","        mask = cv2.resize(mask / mask.max(), im.size)[..., np.newaxis]\n","\n","        result = (mask * im).astype(\"uint8\")\n","\n","        fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 16))\n","        ax1.set_title(\"Original\", fontsize=16)\n","        ax2.set_title(\"Attention Map\", fontsize=16)\n","\n","        ax1.imshow(im)\n","        ax2.imshow(result)\n","\n","        fig.savefig(f\"{config.attentions_plots_dir}/{config.name}_attention_v1.png\")\n","\n","        plt.close()\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","\n","def freeze_layers(model):\n","    for param in model.parameters():\n","        param.requires_grad = False\n","    model.head.weight.requires_grad = True\n","    model.head.bias.requires_grad = True\n","    return model\n","\n","\n","\"\"\" Aug Pretrained Model \"\"\"\n","\n","\n","def vit_base_aug_pretrained():\n","    config = VitBaseAugPretrained()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def vit_large_aug_pretrained():\n","    config = VitLargeAugPretrained()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def vit_huge_aug_pretrained():\n","    config = VitHugeAugPretrained()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","\"\"\" Non Aug Pretrained Model \"\"\"\n","\n","\n","def vit_base_pretrained():\n","    config = VitBasePretrained()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def vit_large_pretrained():\n","    config = VitLargePretrained()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def vit_huge_pretrained():\n","    config = VitHugePretrained()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","\"\"\"Aug Non Pretrained Model \"\"\"\n","\n","\n","def vit_base_aug():\n","    config = VitBaseAug()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def vit_large_aug():\n","    config = VitLargeAug()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def vit_huge_aug():\n","    config = VitHugeAug()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","\"\"\"Non Aug Non Pretrained Model \"\"\"\n","\n","\n","def vit_base():\n","    config = VitBase()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def vit_large():\n","    config = VitLarge()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def vit_huge():\n","    config = VitHuge()\n","    config.device = device\n","    model = VisionTransformer(config)\n","    if config.pretrained == True:\n","        model.load_from(np.load(config.pretrained_dir))\n","        model = freeze_layers(model)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","\"\"\"Res Net 152\"\"\"\n","\n","\n","def resnet_152():\n","    config = Resnet152()\n","    config.device = device\n","    model = models.resnet152(pretrained=config.pretrained)\n","    if config.pretrained == True:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","    model.fc = nn.Linear(2048, config.num_classes)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def resnet_152_aug():\n","    config = Resnet152Aug()\n","    config.device = device\n","    model = models.resnet152(pretrained=config.pretrained)\n","    if config.pretrained == True:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","    model.fc = nn.Linear(2048, config.num_classes)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def resnet_152_aug_pretrained():\n","    config = Resnet152AugPretrained()\n","    config.device = device\n","    model = models.resnet152(pretrained=config.pretrained)\n","    if config.pretrained == True:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","    model.fc = nn.Linear(2048, config.num_classes)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def resnet_152_pretrained():\n","    config = Resnet152Pretrained()\n","    config.device = device\n","    model = models.resnet152(pretrained=config.pretrained)\n","    if config.pretrained == True:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","    model.fc = nn.Linear(2048, config.num_classes)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","\"\"\"Res Net 50\"\"\"\n","\n","\n","def resnet_50():\n","    config = Resnet50()\n","    config.device = device\n","    model = models.resnet50(pretrained=config.pretrained)\n","    if config.pretrained == True:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","    model.fc = nn.Linear(2048, config.num_classes)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def resnet_50_aug():\n","    config = Resnet50Aug()\n","    config.device = device\n","    model = models.resnet50(pretrained=config.pretrained)\n","    if config.pretrained == True:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","    model.fc = nn.Linear(2048, config.num_classes)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def resnet_50_aug_pretrained():\n","    config = Resnet50AugPretrained()\n","    config.device = device\n","    model = models.resnet50(pretrained=config.pretrained)\n","    if config.pretrained == True:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","    model.fc = nn.Linear(2048, config.num_classes)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()\n","\n","\n","def resnet_50_pretrained():\n","    config = Resnet50Pretrained()\n","    config.device = device\n","    model = models.resnet50(pretrained=config.pretrained)\n","    if config.pretrained == True:\n","        for param in model.parameters():\n","            param.requires_grad = False\n","    model.fc = nn.Linear(2048, config.num_classes)\n","    model.to(config.device)\n","    logger.info(f\"Starting Training {config.name}\")\n","    run(config, model)\n","\n","    torch.cuda.empty_cache()"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/home/hensel/weights/ViT-B_16.npz'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m~/projects/vit-masked-face-net/skripsi-v11.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='file:///home/hensel/projects/vit-masked-face-net/skripsi-v11.py?line=1620'>1621</a>\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      <a href='file:///home/hensel/projects/vit-masked-face-net/skripsi-v11.py?line=1621'>1622</a>\u001b[0m     \u001b[0;34m\"\"\"Base Model\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> <a href='file:///home/hensel/projects/vit-masked-face-net/skripsi-v11.py?line=1622'>1623</a>\u001b[0;31m     \u001b[0mvit_base_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      <a href='file:///home/hensel/projects/vit-masked-face-net/skripsi-v11.py?line=1623'>1624</a>\u001b[0m     \u001b[0mvit_base\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      <a href='file:///home/hensel/projects/vit-masked-face-net/skripsi-v11.py?line=1624'>1625</a>\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/projects/vit-masked-face-net/skripsi-v11.py\u001b[0m in \u001b[0;36mvit_base_pretrained\u001b[0;34m()\u001b[0m\n","\u001b[0;32m~/.cache/pypoetry/virtualenvs/skripsi-W-SIb8xI-py3.8/lib/python3.8/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    415\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m             \u001b[0mfid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    418\u001b[0m             \u001b[0mown_fid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/hensel/weights/ViT-B_16.npz'"]}],"source":["if __name__ == \"__main__\":\n","    \"\"\"Base Model\"\"\"\n","    vit_base_pretrained()\n","    vit_base()\n","\n","    \"\"\" Base Aug Model \"\"\"\n","    vit_base_aug_pretrained()\n","    vit_base_aug()\n","\n","    \"\"\" Large Model \"\"\"\n","    vit_large_pretrained()\n","    vit_large()\n","\n","    \"\"\" Large Aug Model \"\"\"\n","    vit_large_aug_pretrained()\n","    vit_large_aug()\n","\n","    \"\"\" Huge Model \"\"\"\n","    vit_huge_pretrained()\n","    vit_huge()\n","\n","    \"\"\" Huge Aug Model \"\"\"\n","    vit_huge_aug_pretrained()\n","    vit_huge_aug()\n","\n","    \"\"\"Resnet 152\"\"\"\n","    resnet_152_pretrained()\n","    resnet_152()\n","\n","    resnet_152_aug_pretrained()\n","    resnet_152_aug()\n","\n","    \"\"\"Resnet 50\"\"\"\n","    resnet_50_pretrained()\n","    resnet_50()\n","\n","    resnet_50_aug_pretrained()\n","    resnet_50_aug()\n","\n","    logger.info(\"Training Finished\")"]}],"metadata":{"interpreter":{"hash":"e1dd5dbc35039269e772ed47a109b225faef5c591e9fc019c28165be159ffa3d"},"kernelspec":{"display_name":"Python 3.8.10 64-bit ('skripsi-W-SIb8xI-py3.8': poetry)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":2}
